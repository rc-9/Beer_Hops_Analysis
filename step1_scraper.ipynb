{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a4bcf35-fc58-4b04-8c8a-f8f6f71b54b8",
   "metadata": {},
   "source": [
    "# Data-Collection from Beer Maverick Hops Database \n",
    "\n",
    "**Source**: https://beermaverick.com/hops/\n",
    "\n",
    "**Method**: Web-Scraping\n",
    "\n",
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e952692b-6679-4419-8895-e65df17ce113",
   "metadata": {},
   "source": [
    "### Setup & Validation\n",
    "**Objective**: Import required modules, establish connection & validate scraping capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63791a63-6c54-4b3b-8930-28bf22917e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "# Import necessary packages\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import unicodedata2\n",
    "import urllib.robotparser\n",
    "from bs4 import BeautifulSoup as bsoup\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84a281e5-2752-4d53-9925-a5462fce4ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use RobotParser to check validity of scraping on endpoint of interest (checking for base page)\n",
    "base_url = 'https://beermaverick.com/hops/'\n",
    "rp = urllib.robotparser.RobotFileParser()\n",
    "rp.set_url('https://beermaverick.com/robots.txt')\n",
    "rp.read()\n",
    "rp.can_fetch('*', base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a98fcae-8568-42bb-a544-c41ec0138dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successfully established. \n",
      "Status Code:  200\n"
     ]
    }
   ],
   "source": [
    "# Establish connection to base-URL & setup a base exception handling\n",
    "try:\n",
    "    response = requests.get(base_url)\n",
    "    print('Connection successfully established. \\nStatus Code: ', response.status_code)\n",
    "except requests.exceptions.RequestException as err:  \n",
    "    print('Unable to establish connection at this time: \\n', err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7259f0ce-1c73-4a51-abdc-f281c0687c50",
   "metadata": {},
   "source": [
    "### Base-URL Scraping\n",
    "**Objective:** Collect hop names & corresponding endpoint URLs for further validation & scraping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25380eb2-bf28-49d0-994a-e2941475cd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Instantiate BS4 for parsing the HTML data \n",
    "soup = bsoup(response.text, 'html')\n",
    "\n",
    "# Find container tag & specific attribute for hops names\n",
    "hops_container = soup.find_all('div', {'class':'box-inner-p-bigger box-single'})[1]  # locate specific container\n",
    "hops_container = hops_container.find_all('p')  # gather each section\n",
    "\n",
    "# Loop through each subsection of container & store obtained hop name/endpoint\n",
    "hop_names = dict()\n",
    "for section in hops_container[:-1]:\n",
    "    all_hops = section.find_all('a')\n",
    "    for each_hop in all_hops:\n",
    "        hop_names[each_hop.text] = each_hop['href']\n",
    "\n",
    "print(len(hop_names) == 304)  # Verify correct amount of hops \n",
    "# print(hop_names)  # Look at final dict (key=official-hop-name & value=url's endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3988a8-61ef-4b23-b472-3251ee6ea592",
   "metadata": {},
   "source": [
    "### Endpoint Scraping\n",
    "**Objective:** Collect & store detailed data for each hop from its respective endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "727cb84e-ed9b-430a-8a3e-fbfe95a09b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hops_scraper(hops):\n",
    "    \n",
    "    # Loop through each hop page and retrive data ('list(' used for debug purposes; to be normalized later)\n",
    "    for hop in list(hops.keys())[0:]:\n",
    "\n",
    "        endpoint_url = 'https://beermaverick.com' + hops[hop]\n",
    "\n",
    "        # Validate connection & force wait-time to prevent Error 429 (too many requests)\n",
    "        try:\n",
    "            response = requests.get(endpoint_url)\n",
    "            sleep(8)\n",
    "        except requests.exceptions.RequestException as err:  \n",
    "            print('Unable to establish connection at this time: \\n', err)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "\n",
    "            hop_dict = {'Hop Name': hop, 'Scraping Status': True}  # initiate dict to store hop data\n",
    "\n",
    "            # Instantiate BS4 & locate umbrella container tag/attribute for all data sections\n",
    "            soup = bsoup(response.text, 'html')\n",
    "            info_section = soup.find('div', {'class':'entry-content'})\n",
    "\n",
    "            # Retrieve info from initial overview table (purpose, country, code, ownership)\n",
    "            table = info_section.find('table')\n",
    "            table = table.find_all('tr')  # each 'tr' tags each row\n",
    "            for row in table:\n",
    "                key = row.find('th').text  # each 'th' tags index col\n",
    "                val = row.find('td').text  # each 'td' tags value col\n",
    "                hop_dict[key] = val\n",
    "\n",
    "            # Retrieve flavor/aroma profile characteristics (displayed as \"tags\" on webpage)\n",
    "            aroma_profile = info_section.find('em')\n",
    "            if aroma_profile != None:  # conditional check if sectino exists\n",
    "                aroma_profile = aroma_profile.find_all('a', {'class':'text-muted'})\n",
    "                val = [tag.text for tag in aroma_profile]\n",
    "                hop_dict['Flavor & Aroma Profile'] = val\n",
    "\n",
    "            # Retrieve data from brewing values table\n",
    "            brew_table = info_section.find('table', {'class':'brewvalues'})\n",
    "            brew_table = brew_table.find_all('tr')  # each 'tr' tags each row\n",
    "            for row in brew_table:\n",
    "                # Conditional to skip over 'Total Oil Breadown' empty row without values\n",
    "                if row.find('th') != None:\n",
    "                    # Grab key-value pair of the brewing value for that row\n",
    "                    key = row.find('th').text.replace(row.find('small').text, '')  # subtracting details-subtext from heading str\n",
    "                    val = row.find('td').text  # each 'td' tags value col\n",
    "                    hop_dict[key] = val\n",
    "                   \n",
    "        else:\n",
    "            hop_dict = {'Hop Name': hop, 'Scraping Status': False}\n",
    "        \n",
    "        # print(hop_dict)  # debug\n",
    "        yield hop_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2453508-508a-493d-890d-de7f17327b6f",
   "metadata": {},
   "source": [
    "### Load Hops Data\n",
    "**Objective**: Execute scraper generator function and append each yield dict as row into dataframe & output CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15cb98fe-a7e3-4cce-8482-fe7d85d8d09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2769.429688692093\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "t1 = time()\n",
    "hops_df = pd.DataFrame(hops_scraper(hop_names))\n",
    "t2 = time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3985aa82-9cc5-4a07-9a28-6edefd159b90",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './rc_raw_data/rc_raw_hops_main.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_750/35649741.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhops_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./rc_raw_data/rc_raw_hops_main.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhops_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3385\u001b[0m         )\n\u001b[1;32m   3386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3387\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3388\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3389\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1081\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m         )\n\u001b[0;32m-> 1083\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \"\"\"\n\u001b[1;32m    227\u001b[0m         \u001b[0;31m# apply compression and byte/text conversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         with get_handle(\n\u001b[0m\u001b[1;32m    229\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './rc_raw_data/rc_raw_hops_main.csv'"
     ]
    }
   ],
   "source": [
    "hops_df.to_csv('./raw_data/raw_hops_main.csv')\n",
    "hops_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c09d74-6a23-4f7f-9ea2-47618fbe020b",
   "metadata": {},
   "source": [
    "### Reference-Material Scraping\n",
    "**Objective**: Scrape reference materials (meta-data / standard info) for further context & analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bbf854-32a4-49c6-90db-6ce785e95f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SCRAPE HOPS-SUBSTITUTION CHART\n",
    "\n",
    "substitutions_url = 'https://beermaverick.com/hops/hop-substitutions-chart/'\n",
    "\n",
    "# Validate connection\n",
    "try:\n",
    "    response = requests.get(substitutions_url)\n",
    "except requests.exceptions.RequestException as err:  \n",
    "    print('Unable to establish connection at this time: \\n', err)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    \n",
    "    # Initiate list with empty dict objects for each row of data loading\n",
    "    subs_info = [{} for i in range(214)]  # for each row on webpage\n",
    "\n",
    "    # Instantiate BS4 & locate container tag/attribute for table\n",
    "    soup = bsoup(response.text, 'html')\n",
    "    table = soup.find('tbody')\n",
    "    table = table.find_all('tr')  # each 'tr' tags each row\n",
    "    \n",
    "    # Loop through each row of table and load info to corresponding dict in list\n",
    "    row_num = 0\n",
    "    for row in table:\n",
    "        subs_info[row_num]['Hop Name'] = row.find('th').text  # each 'th' tags index col\n",
    "        subs_info[row_num]['Substitutions'] = row.find('td').text  # each 'td' tags value col\n",
    "        row_num += 1\n",
    "\n",
    "# Load scraped data into local df & output CSV\n",
    "subs_df = pd.DataFrame(subs_info)\n",
    "subs_df.to_csv('./raw_data/raw_ref_hops_substitutions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bd4b1c-0cfa-498f-9af7-6af7793eef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1287c782-82f0-46dd-9441-262ccc162a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SCRAPE AROMAS META-DATA\n",
    "\n",
    "aroma_meta_url = 'https://beermaverick.com/the-science-behind-identifying-hop-aromas/'\n",
    "\n",
    "# Validate connection\n",
    "try:\n",
    "    response = requests.get(aroma_meta_url)\n",
    "except requests.exceptions.RequestException as err:  \n",
    "    print('Unable to establish connection at this time: \\n', err)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    \n",
    "    # Initiate lists of dicts for each aroma type, to be used for data loading\n",
    "    aromas_info = [\n",
    "        {'Aroma Type': 'Floral'},\n",
    "        {'Aroma Type': 'Citrus'},\n",
    "        {'Aroma Type': 'Tropical/Sweet Fruits'},\n",
    "        {'Aroma Type': 'Stone/Green Fruits'},\n",
    "        {'Aroma Type': 'Berries & Currant'},\n",
    "        {'Aroma Type': 'Cream & Caramel'},\n",
    "        {'Aroma Type': 'Woody Aromatic'},\n",
    "        {'Aroma Type': 'Menthol'},\n",
    "        {'Aroma Type': 'Herbal'},\n",
    "        {'Aroma Type': 'Spicy'},\n",
    "        {'Aroma Type': 'Grassy'},\n",
    "        {'Aroma Type': 'Vegetal'}\n",
    "    ]\n",
    "    \n",
    "    # Instantiate BS4 & locate container tag/attribute aroma info\n",
    "    soup = bsoup(response.text, 'html')\n",
    "    aroma_section = soup.find('div', {'class':'entry-content'})\n",
    "    aroma_section = aroma_section.find_all('li')\n",
    "    \n",
    "    # Separate aroma types & compound names information\n",
    "    aroma_types = aroma_section[0:-1:2]\n",
    "    compound_names = aroma_section[1:-1:2]\n",
    "    \n",
    "    # Loop through each aroma type & compound name and load info to corresponding dict in list\n",
    "    for i in range(len(aromas_info)):\n",
    "        aromas_info[i][aroma_types[i].text[0:6]] = aroma_types[i].text[8:]\n",
    "        aromas_info[i][compound_names[i].text[0:21]] = compound_names[i].text[23:]\n",
    "\n",
    "# Load scraped data into local df & output CSV\n",
    "aromas_df = pd.DataFrame(aromas_info)\n",
    "aromas_df.to_csv('./raw_data/raw_ref_aroma_types.csv')\n",
    "aromas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219ee710-ef35-4042-8fde-a256913f174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SCRAPE BREWING VALUES META-DATA \n",
    "\n",
    "brew_meta_url = 'https://beermaverick.com/hop/newport/'  # using a hops page with all brew values\n",
    "\n",
    "# Validate connection\n",
    "try:\n",
    "    response = requests.get(brew_meta_url)\n",
    "except requests.exceptions.RequestException as err:  \n",
    "    print('Unable to establish connection at this time: \\n', err)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    \n",
    "    meta = []\n",
    "    \n",
    "    # Instantiate BS4 & locate umbrella container tag/attribute for all data sections\n",
    "    soup = bsoup(response.text, 'html')\n",
    "    info_section = soup.find('div', {'class':'entry-content'})\n",
    "    \n",
    "    # Retrieve data from brewing values table\n",
    "    brew_table = info_section.find('table', {'class':'brewvalues'})\n",
    "    brew_table = brew_table.find_all('tr')  # each 'tr' tags each row\n",
    "    for row in brew_table:\n",
    "        # Conditional to skip over 'Total Oil Breadown' empty row without values\n",
    "        if row.find('th') != None:\n",
    "            # Grab info from first column for each row of brew values\n",
    "            brew_type = row.find('th').text.replace(row.find('small').text, '')  # subtracting details-subtext from heading str\n",
    "            brew_info = row.find('th').find('small').text  \n",
    "            meta.append({'Value Type': brew_type, 'Description': brew_info})\n",
    "\n",
    "# Load scraped data into local df & output CSV\n",
    "brew_meta_df = pd.DataFrame(meta)\n",
    "brew_meta_df.to_csv('./raw_data/raw_ref_brew_values.csv')\n",
    "brew_meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0bbb82-cc3d-4c54-9517-cb79a7411b12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
